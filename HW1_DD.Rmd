---
title: 'DATA 621: HW1'
output:
  pdf_document: default
---



# Data Exploration

## Correlations

We can start by plotting a correlation matrix, describing the pearson correlation coefficients between predictors. This will inform us about our dataset and provide insight when model building:

```{r, echo = F, warning = F, message = F}

library(tidyverse)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(knitr)
library(broom)

df.training = read_csv('moneyball-training-data.csv')

mat.correlation = cor(df.training, use = 'complete')
mat.correlation[upper.tri(mat.correlation)] = NA
mlt.correlation <- melt(mat.correlation)


ggplot(data = mlt.correlation, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 8, hjust = 1), axis.text.y = element_text(size = 8))+
 coord_fixed()
```

Both strikeouts by batters (`TEAM_BATTING_SO`) and errors (`TEAM_FIELDING_E`) are negatively correlated with the number of wins, while base hits by batters (`TEAM_BATTING_H`) is positively associated with the total win count.

We can further visualize the correlations against `TARGET_WINS` using scatterplots:

```{r, echo = F, warning = F, message = F}

mlt.training.plt = melt(select(df.training, -c("TARGET_WINS")), id.vars = c("INDEX"))
mlt.training.plt = merge(mlt.training.plt, select(df.training, c("INDEX", "TARGET_WINS")), by = "INDEX")
ggplot(data = mlt.training.plt, aes(value, TARGET_WINS)) + geom_point() + facet_wrap(~variable, scales = "free")
```


# Data Distribution

## Distributions

We can visualize the variables using histograms to account for non-normal distributions:

```{r, echo = F, warning = F, message = F}

mlt.training = melt(df.training, id.vars = "INDEX")
ggplot(data = mlt.training, aes(value)) + geom_histogram() + facet_wrap(~variable, scales = "free")
```

One of the key takeaways here is that strikeouts by batters has a bimodal distribution. Several variables, such as strikeouts by pitchers and walks allowed are skewed right. `TARGET_WINS` (i.e. our outcome of interest) has a normal distribution.



# Basic Model

## Inclued all predictors. 

Create a model and show the p-values for each estimate.
```{r, echo = F, warning = F, message = F}
lmod <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)

# coef(lmod)

(lmod)   %>%   broom::tidy() %>% kable()

par(mfrow = c(2, 3))
plot(lmod)
```

We see that linearity seems to hold reasonably well, as the red line is close to the dashed line. We can also note the heteroskedasticity in the data, as we move to the edges on the x-axis, the spread of the residuals seems to be increasing. Finally, points 1188, 975, and 21340 may be outliers, with large residual values.


Let's perform other checks for Normality. We can investigate using a density plot

```{r, echo = F, warning = F, message = F}
d<-density(lmod[['residuals']])
plot(d,main='Residual Plot',xlab='Residual value')
```

Looking at the above density plot, there appears to be some negative skewness on the right, appearing like a fat tail. Let's try an empirical CDF plot:

```{r, echo = F, warning = F, message = F}
	
plot(ecdf(lmod[['residuals']]))

```

It's difficult to conclude much with the above plot. Let's try log transformation of data

```{r, echo = F, warning = F, message = F}
lmodel <- lm(log(TARGET_WINS) ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)
  

library(moments)

skewness(lmodel$residuals)
lmodel %>%   broom::tidy() %>% kable()

par(mfrow = c(2, 3))
plot(lmodel)
```

The qq plot looks little better after the transformation. Let check the density plot after transformation

```{r, echo = F, warning = F, message = F}
ld<-density(lmodel[['residuals']])
plot(ld,main='Residual Plot',xlab='Residual value')
```


This is slightly better than the previous case, but not by much.


# Data Transformation: Model using sqrt transformation

```{r, echo = F, warning = F, message = F}
l2model <- lm(sqrt(TARGET_WINS) ~ ., df.training)
  
l2model  %>%   broom::tidy() %>% kable()

par(mfrow= c(2,2))
hist(l2model$residuals)

qqnorm(l2model$residuals)
qqline(l2model$residuals)

#plot(l2model)
```


```{r, echo = F, warning = F, message = F}

library(moments)

skewness(l2model$residuals)

```


# Data Transformation: Model using BOXCOX transformation

We can compare this with other model:

```{r, echo = F, warning = F, message = F}

library(MASS)

model0 <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)


model <- lm(sqrt(TARGET_WINS) ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)
             
y <- df.training$TARGET_WINS
x <-  df.training$TEAM_BATTING_H + df.training$TEAM_BATTING_2B + df.training$TEAM_BATTING_3B + df.training$TEAM_BATTING_HR + df.training$TEAM_BATTING_BB + df.training$TEAM_BATTING_SO + df.training$TEAM_BASERUN_SB + df.training$TEAM_BASERUN_CS + df.training$TEAM_BATTING_HBP + df.training$TEAM_PITCHING_H + df.training$TEAM_PITCHING_HR + df.training$TEAM_PITCHING_BB + df.training$TEAM_PITCHING_SO + df.training$TEAM_FIELDING_E + df.training$TEAM_FIELDING_DP

#find optimal lambda for Box-Cox transformation 
bc_model <- boxcox(y ~ x)

(lambda <- bc_model$x[which.max(bc_model$y)])

#fit new linear regression model using the Box-Cox transformation
new_model <- lm(((y^lambda-1)/lambda) ~ x)

(new_model)   %>%   broom::tidy() %>% kable()

################################################

#define plotting area
op <- par(pty = "s", mfrow = c(1, 3))

#Q-Q plot for original model
qqnorm(model0$residuals)
qqline(model0$residuals)


#Q-Q plot for sqrt model
qqnorm(model$residuals)
qqline(model$residuals)

#Q-Q plot for Box-Cox transformed model
qqnorm(new_model$residuals)
qqline(new_model$residuals)

#display both Q-Q plots
par(op)

```

### Trying to study model by subsetting the data

```{r, echo = F, warning = F, message = F}

library(MASS)

y1 <- df.training$TARGET_WINS
x1 <- df.training$TEAM_BATTING_H + df.training$TEAM_BATTING_2B
```


\newpage

# Appendix with R Code

```{r, eval = F, warning = F, message = F}

library(tidyverse)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(knitr)
library(broom)

df.training = read_csv('moneyball-training-data.csv')

# Correlation
mat.correlation = cor(df.training, use = 'complete')
mat.correlation[upper.tri(mat.correlation)] = NA
mlt.correlation <- melt(mat.correlation)


ggplot(data = mlt.correlation, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 8, hjust = 1), axis.text.y = element_text(size = 8))+
 coord_fixed()
```


```{r, eval = F, warning = F, message = F}
lmod <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)

# coef(lmod)

(lmod)   %>%   broom::tidy() %>% kable()

par(mfrow = c(2, 3))
plot(lmod)

d<-density(lmod[['residuals']])
plot(d,main='Residual Plot',xlab='Residual value')


plot(ecdf(lmod[['residuals']]))

lmodel <- lm(log(TARGET_WINS) ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)
  

library(moments)

skewness(lmodel$residuals)
lmodel %>%   broom::tidy() %>% kable()

par(mfrow = c(2, 3))
plot(lmodel)

ld<-density(lmodel[['residuals']])
plot(ld,main='Residual Plot',xlab='Residual value')

l2model <- lm(sqrt(TARGET_WINS) ~ ., df.training)
  
l2model  %>%   broom::tidy() %>% kable()

par(mfrow= c(2,2))
hist(l2model$residuals)

qqnorm(l2model$residuals)
qqline(l2model$residuals)

#plot(l2model)

library(moments)

skewness(l2model$residuals)

library(MASS)

model0 <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)


model <- lm(sqrt(TARGET_WINS) ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)
             
y <- df.training$TARGET_WINS
x <-  df.training$TEAM_BATTING_H + df.training$TEAM_BATTING_2B + df.training$TEAM_BATTING_3B + df.training$TEAM_BATTING_HR + df.training$TEAM_BATTING_BB + df.training$TEAM_BATTING_SO + df.training$TEAM_BASERUN_SB + df.training$TEAM_BASERUN_CS + df.training$TEAM_BATTING_HBP + df.training$TEAM_PITCHING_H + df.training$TEAM_PITCHING_HR + df.training$TEAM_PITCHING_BB + df.training$TEAM_PITCHING_SO + df.training$TEAM_FIELDING_E + df.training$TEAM_FIELDING_DP

#find optimal lambda for Box-Cox transformation 
bc_model <- boxcox(y ~ x)

(lambda <- bc_model$x[which.max(bc_model$y)])

#fit new linear regression model using the Box-Cox transformation
new_model <- lm(((y^lambda-1)/lambda) ~ x)

(new_model)   %>%   broom::tidy() %>% kable()

################################################

#define plotting area
op <- par(pty = "s", mfrow = c(1, 3))

#Q-Q plot for original model
qqnorm(model0$residuals)
qqline(model0$residuals)


#Q-Q plot for sqrt model
qqnorm(model$residuals)
qqline(model$residuals)

#Q-Q plot for Box-Cox transformed model
qqnorm(new_model$residuals)
qqline(new_model$residuals)

#display both Q-Q plots
par(op)

library(MASS)

y1 <- df.training$TARGET_WINS
x1 <- df.training$TEAM_BATTING_H + df.training$TEAM_BATTING_2B
```
