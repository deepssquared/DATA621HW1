---
title: "DATA_621_HW1"
output:
  html_document: default
  word_document: default
  pdf_document: default
date: "2022-09-13"
---


##### Let's load the required libraries in R for data analysis
```{r load-packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(knitr)
```



```{r, echo = F, warning = F, message = F}

df.training = read.csv('https://raw.githubusercontent.com/deepssquared/DATA621HW1/main/moneyball-training-data.csv')
  
#  head(df.training)


# Correlation
mat.correlation = cor(df.training, use = 'complete')
mat.correlation[upper.tri(mat.correlation)] = NA

mlt.correlation <- melt(mat.correlation)

#ggplot(data = mlt.correlation, aes(Var2, Var1, fill = value))
#  + geom_tile(color = "white")
#  + scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation") 
#  + theme_minimal() 
#  + theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1), axis.text.y = element_text(size = 8))
#  + coord_fixed()

```

### Both strikeouts by batters (`TEAM_BATTING_SO`) and errors (`TEAM_FIELDING_E`) are negatively correlated with the number of wins, while base hits by batters (`TEAM_BATTING_H`) is positively associated with the total win count.

### We can further visualize the correlations against `TARGET_WINS` using scatterplots:

```{r, echo = F, warning = F, message = F}

mlt.training.plt = melt(select(df.training, -c("TARGET_WINS")), id.vars = c("INDEX"))
mlt.training.plt = merge(mlt.training.plt, select(df.training, c("INDEX", "TARGET_WINS")), by = "INDEX")
ggplot(data = mlt.training.plt, aes(value, TARGET_WINS)) + geom_point() + facet_wrap(~variable, scales = "free")

```

## Distributions

We can visualize the variables using histograms to account for non-normal distributions:

```{r, echo = F, warning = F, message = F}

mlt.training = melt(df.training, id.vars = "INDEX")
ggplot(data = mlt.training, aes(value)) + geom_histogram() + facet_wrap(~variable, scales = "free")
```

One of the key takeaways here is that strikeouts by batters has a bimodal distribution. Several variables, such as strikeouts by pitchers and walks allowed are skewed right. `TARGET_WINS` (i.e. our outcome of interest) has a normal distribution.


### Inclued all predictors. Create a model and show the p-values for each estimate.
```{r}
lmod <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)

```

```{r}
coef(lmod)

summary(lmod)

plot(lmod)
```

#### We see that linearity seems to hold reasonably well, as the red line is close to the dashed line. 
#### We can also note the heteroskedasticity in the data, as we move to the edges on the x-axis, the spread of the residuals seems to be increasing. Finally, points 1188, 975, and 21340 may be outliers, with large residual values.


### Let's perform other checks for Normality

#### We can investigate using a density plot

```{r}
d<-density(lmod[['residuals']])
plot(d,main='Residual Plot',xlab='Residual value')

```

#### Looking at the above density plot, there appears to be some negative skewness on the right, appearing like a fat tail.

#### Let's try an empirical CDF plot 
```{r}
	
plot(ecdf(lmod[['residuals']]))

```

#### It's difficult to conclude much with the above plot.

### Let's try log transformation of data

```{r}
lmodel <- lm(log(TARGET_WINS) ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)
  

library(moments)

skewness(lmodel$residuals)


summary(lmodel)


plot(lmodel)
```

#### The qq plot looks little better after the transformation

#### Let check the density plot after transformation

```{r}
ld<-density(lmodel[['residuals']])
plot(ld,main='Residual Plot',xlab='Residual value')

```


#### This is slightly better than the previous case, but not by much.


### Let's create model using sqrt transformation

```{r}
l2model <- lm(sqrt(TARGET_WINS) ~ ., df.training)
  
summary(l2model)

par(mfrow= c(2,1))
hist(l2model$residuals)

qqnorm(l2model$residuals)
qqline(l2model$residuals)

#plot(l2model)
```
```{r}

library(moments)

skewness(l2model$residuals)

```


### Let's create model using BOXCOX transformation and compare with other model

```{r}

library(MASS)

model0 <- lm(TARGET_WINS ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)


model <- lm(sqrt(TARGET_WINS) ~ TEAM_BATTING_H+TEAM_BATTING_2B+TEAM_BATTING_3B+TEAM_BATTING_HR+TEAM_BATTING_BB+TEAM_BATTING_SO+TEAM_BASERUN_SB+TEAM_BASERUN_CS+TEAM_BATTING_HBP+TEAM_PITCHING_H+TEAM_PITCHING_HR+TEAM_PITCHING_BB+TEAM_PITCHING_SO+TEAM_FIELDING_E+TEAM_FIELDING_DP, df.training)
             
y <- df.training$TARGET_WINS
x <-  df.training$TEAM_BATTING_H + df.training$TEAM_BATTING_2B + df.training$TEAM_BATTING_3B + df.training$TEAM_BATTING_HR + df.training$TEAM_BATTING_BB + df.training$TEAM_BATTING_SO + df.training$TEAM_BASERUN_SB + df.training$TEAM_BASERUN_CS + df.training$TEAM_BATTING_HBP + df.training$TEAM_PITCHING_H + df.training$TEAM_PITCHING_HR + df.training$TEAM_PITCHING_BB + df.training$TEAM_PITCHING_SO + df.training$TEAM_FIELDING_E + df.training$TEAM_FIELDING_DP

#find optimal lambda for Box-Cox transformation 
bc_model <- boxcox(y ~ x)

(lambda <- bc_model$x[which.max(bc_model$y)])

#fit new linear regression model using the Box-Cox transformation
new_model <- lm(((y^lambda-1)/lambda) ~ x)

summary(new_model)

################################################

#define plotting area
op <- par(pty = "s", mfrow = c(1, 3))

#Q-Q plot for original model
qqnorm(model0$residuals)
qqline(model0$residuals)


#Q-Q plot for sqrt model
qqnorm(model$residuals)
qqline(model$residuals)

#Q-Q plot for Box-Cox transformed model
qqnorm(new_model$residuals)
qqline(new_model$residuals)

#display both Q-Q plots
par(op)

```

### Trying to study model by subsetting the data

```{r}

library(MASS)


             
y1 <- df.training$TARGET_WINS
x1 <- df.training$TEAM_BATTING_H + df.training$TEAM_BATTING_2B

#find optimal lambda for Box-Cox transformation 
# bc_model1 <- boxcox(y1 ~ x1)
#bc_model1 <- boxcox( df.training$TARGET_WINS ~ (df.training$TEAM_BATTING_H + df.training$TEAM_BATTING_2B) )

#(lambda1 <- bc_model1$x1[which.max(bc_model1$y1)])

#fit new linear regression model using the Box-Cox transformation
#new_model1 <- lm(((y1^lambda1 - 1)/lambda1) ~ x1)

#summary(new_model1)

```

